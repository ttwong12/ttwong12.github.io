<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/vlipp/vlipp.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=300 src="papers/vlipp/images/icon.gif" border=0></td>      
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   VLIPP: Towards Physically Plausible Video Generation</b>
   <i><b> (ICCV 2025)</b></i></font><br> 

Most current video-generation systems struggle to make motions look physically realistic, especially over longer periods of time. This is likely because they are trained on short video clips and the model sizes are limited. In this work, we use VLM and LLM to reason about basic physical rules and predict how motion should evolve over time. These long-term motion predictions are then used to guide the video generation process, while the video model itself focuses on generating realistic short-term motion and visual details.


<br>
<b>
[<a href="https://madaoer.github.io/projects/physically_plausible_video_generation/">more</a>]

<!-- backup of project webpage 
[<a href="./papers/vlipp/vlipp.html">more</a>]-->

<!-- arVix paper as backup -->
[<a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_VLIPP_Towards_Physically_Plausible_Video_Generation_with_Vision_and_Language_ICCV_2025_paper.pdf">paper</a>]

[<a href="https://github.com/Madaoer/VLIPP">code</a>]
</td></tr></table>
<p><br>