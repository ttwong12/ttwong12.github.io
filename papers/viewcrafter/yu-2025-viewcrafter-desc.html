<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/viewcrafter/viewcrafter.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=300 src="papers/viewcrafter/images/icon.gif" border=0></td>      
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis</b>
   <i><b> (IEEE TPAMI 2025)</b></i></font><br> 

Many video models are good at creating smooth and visually pleasing novel views, but they do not guarantee the consistency of the underlying 3D structure over time. In this project, we improve consistency by first estimating a rough 3D shape (point cloud) of the scene from the reference view. We then use this 3D representation to render how the scene should look from a new viewpoint. These rendered views may be incomplete, but they provide reliable geometric guidance. By conditioning the video generation with the point cloud renders, the model produces novel views that better preserve the true shape of the scene. This procedure can be repeated to progressively generate longer or more complex videos, as well as extending the point cloud geometry, while maintaining geometric coherence.


<br>
<b>
[<a href="https://drexubery.github.io/ViewCrafter/">more</a>]

<!-- backup of project webpage 
[<a href="./papers/viewcrafter/viewcrafter.html">more</a>]-->

[<a href="https://doi.org/10.1109/TPAMI.2025.3613256">paper</a>]

<!-- arVix paper as backup -->
[<a href="https://arxiv.org/abs/2409.02048">arxiv</a>]

[<a href="https://github.com/Drexubery/ViewCrafter">code</a>]
</td></tr></table>
<p><br>