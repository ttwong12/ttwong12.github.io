<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="./papers/mvd/mvd.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=300 src="papers/mvd/images/mvd.gif" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   Text-Guided Texturing by Synchronized Multi-View Diffusion </b>
   <i><b> (SIGGRAPH Asia 2024)</b></i></font><br> 

Texturing a mesh could be very tedious and labor intensive. There is nothing
better than dressing an object by simply typing a few words. By utilizing the 
image prior of a pretrained text-to-image diffusion
model, we can synthesize high-quality textures for any given object mesh.
The key challenge lies on how to produce a high-quality 360&deg;-complete,
consistent and plausible textures, without
over-fragmentation, obvious seams, and over-blurriness. The proposed synchronized 
multi-view diffusion allows faster convergence during the denoising process.


<br>
<b>
[<a href="https://github.com/LIU-Yuxin/SyncMVD">more</a>]
[<a href="https://arxiv.org/abs/2311.12891">paper</a>]
[<a href="https://github.com/LIU-Yuxin/SyncMVD">code</a>]
</td></tr></table>
<p><br>