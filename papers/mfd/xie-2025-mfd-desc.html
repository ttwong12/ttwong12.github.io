
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="./papers/mfd/mfd.html"-->
  <img width=300 src="papers/mfd/images/car.gif" border=0></td>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   Synchronized Multi-Frame Diffusion for Temporally Consistent Video Stylization </b>
   <i><b> (EG2025)</b></i></font><br> 

Video stylization converts a given video into the desired style or effect. 
Existing text-to-image diffusion models offer a convenient and high-quality
style transfer for still picture by just typing a text prompt. This work utilizes an image 
diffusion model (instead of a video one) to stylize a video. By assigning a
diffusion process for each frame and sychronized among them, we are able to
synthesize temporal consistent in all visual scales, rich detailed, and
sharp video stylization.


<br>
<b>
[<!--a href="./papers/mfd/mfd.html"-->more</a>]
[<!--a href=""-->paper</a>]
[<!--a href=""-->code</a>]
</td></tr></table>
<p><br>