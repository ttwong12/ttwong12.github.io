<html>
<title>My Favorite Publication</title>
<body background="./icon/desktop.jpg">

<blockquote>

<font face="Verdana">
<h2>My Favorite Works</h2>

<blockquote>
<a href="publication.html">Publication (Recent & Selected)</a><br>
<a href="publication-year.html">Full Publication by Year</a><br>
</blockquote>

<font face="Verdana" size="2">
The titles below may not be exactly the same as in the papers.
They are used  in order to highlight the key ideas/contributions.
Click [<u>more</u>] for details of the paper. Click [<u>paper</u>] to
download the PDF right away. Click [<u>video</u>] for the high-quality project video. 
For moderate quality video on youtube, please click [<u>youtube</u>].
Click [<u>software</u>] for the associated software. Click [<u>press</u>]
for related press clipping.
</font>


<br>
<p>

<! 
  Sorting Criteria:

  There is no specific sorting criteria. I just highlight my major research.

>



<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/tooncrafter/tooncrafter.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=149 src="papers/tooncrafter/images/crossfade.gif" border=0></td>      
	  <td border=0><img width=149 src="papers/tooncrafter/images/anime.gif" border=0></td>      
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   ToonCrafter: Generative Cartoon Interpolation </b>
   <i><b> (SIGGRAPH Asia 2024)</b></i></font><br> 

After decades of searching generic solution for cartoon inbetweening &#128557;, it seems we 
are one more step closer to the ultimate solution. The idea is to exploit the motion priors of 
video diffusion model. Comparing to the costly handdrawn cartoon animation, live-action videos 
are cheaper to acquire and richer in all kinds of motion. This means it is more sensible to 
adapt a live-action video trained model (DynamiCrafter in our case) for cartoon interpolation,
than training one from scratch. The key challenge is how to adapt to the visual appearance of 
cartoons (without generating non-cartoon content) while retaining the motion.

<br>
<b>
<!--[<a href="https://ttwong12.github.io/papers/tooncrafter/tooncrafter.html"more</a>]-->
[<a href="https://doubiiu.github.io/projects/ToonCrafter/">more</a>]
[<a href="https://arxiv.org/abs/2405.17933">paper</a>]
[<a href="https://github.com/ToonCrafter/ToonCrafter">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/dynamicrafter/dynamicrafter.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=149 src="papers/dynamicrafter/images/bear.png" border=0></td>
      <td border=0><img width=149 src="papers/dynamicrafter/images/bear.gif" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   DynamiCrafter: Animating Images with Video Diffusion Priors </b>
   <i><b> (ECCV 2024)</b></i></font><br> 

There has long been a need to animate images, but early works are limited to animating specific
types of motion, such as water or candlelight. However, animating the general motion of an 
arbitrary image remains an open problem. By utilizing motion priors of a text-to-video diffusion
model, we now can animate any still picture with realistic and natural motion.


<br>
<b>
<!--[<a href="https://ttwong12.github.io/papers/dynamicrafter/dynamicrafter.html"more</a>]-->
[<a href="https://doubiiu.github.io/projects/DynamiCrafter/">more</a>]
[<a href="https://arxiv.org/abs/2310.12190">paper</a>]
[<a href="https://github.com/Doubiiu/DynamiCrafter">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="./papers/mvd/mvd.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=300 src="papers/mvd/images/mvd.gif" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   Text-Guided Texturing by Synchronized Multi-View Diffusion </b>
   <i><b> (SIGGRAPH Asia 2024)</b></i></font><br> 

Texturing a mesh could be very tedious and labor intensive. There is nothing
better than dressing an object by simply typing a few words. By utilizing the 
image prior of a pretrained text-to-image diffusion
model, we can synthesize high-quality textures for any given object mesh.
The key challenge lies on how to produce a high-quality 360&deg;-complete,
consistent and plausible textures, without
over-fragmentation, obvious seams, and over-blurriness. The proposed synchronized 
multi-view diffusion allows faster convergence during the denoising process.


<br>
<b>
[<a href="https://github.com/LIU-Yuxin/SyncMVD">more</a>]
[<a href="https://arxiv.org/abs/2311.12891">paper</a>]
[<a href="https://github.com/LIU-Yuxin/SyncMVD">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/scenegen/scenegen.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=149 src="papers/scenegen/images/scene1.jpg" border=0></td>
      <td border=0><img width=149 src="papers/scenegen/images/scene2.jpg" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  </b>
   <i><b>Physics-based Scene Layout Generation from Human Motion (SIGGRAPH 2024)</b></i></font><br> 
  As character motion is often captured in a blue-screened studio without real furniture or objects in place,,
  there may be a discrepancy between the planned motion and the captured one. Hence, the scene/furniture may have to be adjusted accordingly.
  Given a character motion, our method selects the approprate furnitures
  from a library to fit the motion in a physically plausible fashion. The scene layout is simultaneously optimized 
  with the physics based motion simulation to determine the optimal scene layout.
  


<br>
<b>
[<a href="https://jiann-li.github.io/physcenegen/">more</a>]
[<a href="https://arxiv.org/abs/2405.12460">paper</a>]
[code]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="https://doubiiu.github.io/projects/Make-Your-Video/">
  <img width=300 height=150 src="papers/makevideo/images/makevideo-icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Make-Your-Video </b>
  <i><b> (IEEE TVCG 2024)</b></i></font><br> 
  <p>
  Make-Your-Video takes an input depth video or normal video (hence, estimated depth) 
  and the user' text description as guidiances, 
  to generate the desired video.  
  It can significantly improve the temporal consistency while 
  maintain the visual quality of the generated video.
  

<br>

<b>
[<a href="https://doubiiu.github.io/projects/Make-Your-Video/">more</a>]
[<a href="https://arxiv.org/abs/2306.00943">paper (arXiv)</a>]
[<a href="https://github.com/AILab-CVC/Make-Your-Video">code</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="https://doubiiu.github.io/projects/aidn/">
  <img src="papers/invscale/images/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Scale-Arbitrary Invertible Image Downscaling </b>
  <i><b> (IEEE TIP 2023)</b></i></font><br> 
  <p>
  Have you ever complained your image being degraded after posting over
  whatsapp, telegram, or facebook? This is because social media platforms
  usually scale down your image if its resolution is too large. Here,
  we provide a solution of encoding arbitrary high-resolution image details into the
  low-resolution ones, so that after posting the low-resolution images 
  on these social media platforms, we can still restore the high-resolution
  images and their details. Obviously, this technology  can also be 
  adopted by the social media platforms, to retain the high-resolution
  details of users' images without paying extra storage cost.


<br>

<b>
[<a href="https://doubiiu.github.io/projects/aidn/">more</a>]
[<a href="https://ieeexplore.ieee.org/document/10192538">paper</a>]
[<a href="https://github.com/Doubiiu/AIDN">code</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/codetalker/codetalker.html">
  <img width=300 src="papers/codetalker/images/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   CodeTalker: Speech-Driven 3D Facial Animation</b>
   <i><b> (CVPR 2023)</b></i></font><br> 
  <p>

While speech-driven 3D facial animation has been heavily studied, there remains
a gap to achieving expressive and dramatic animation, due to the highly ill-posed
nature and scarcity of audiovisual data.  The unique contribution here is on
minimizing the over-smoothed facial expression using a learned discrete
motion prior, which means more dramatic and expressive facial motions with
more accurate lip movements can be achieved.  Such method is not only useful in virtual reality,
games and film productions, but also is beneficial to general non-professional users
without much animation skill.

<br>
<b>
[<a href="./papers/codetalker/codetalker.html">more</a>]
[<a href="https://doubiiu.github.io/projects/codetalker/">more 2</a>]
[<a href="./papers/codetalker/codetalker.pdf">paper</a>]
[<a href="./papers/codetalker/codetalker.mp4">video</a>]
[<a href="https://github.com/Doubiiu/CodeTalker">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/disco/disco.html">
  <img width=300 src="papers/disco/images/icon.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Disentangled Image Colorization</b>
  <i><b> (SIGGRAPH Asia 2022)</b></i></font><br> 
  <p>
Existing neural-based colorization methods usually fail to consistently colorize, e.g. the dress of a
person. The typical result is the dress is partly in green and partly in gray
even the grayscale dress looks like in the same color. The core of the
problem lies on the difficulty in maintaining the long-distance color
consistency for region that can be colorized in "any" color. To ease the learning, we propose to disentangle the color
multimoality from the structure consistency. So that, a person wearing the
single color dress will be colorized in single color.

<br>

<b>
[<a href="./papers/disco/disco.html"-->more</a>]
[<a href="https://menghanxia.github.io/projects/disco.html"-->more 2</a>]
[<a href="./papers/disco/disco.pdf">paper</a>]
[<a href="https://www.youtube.com/watch?v=Zb5F0449THA">youtube</a>]
[<a href="https://github.com/MenghanXia/DisentangledColorization">code</a>]
[<a href="https://huggingface.co/spaces/menghanxia/disco">demo</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/dashline/dashline.html">
  <img width=300 src="./papers/dashline/images/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Dashed Curves Vectorization</b>
  <i><b> (CVPR 2022)</b></i></font><br> 
  <p>
Dashed lines/curves vectorization has never been successfully handled so
far. We take one step further to vectorize dashed curves using an end-to-end neural
approach. Instead of exporting multiple disjoint line segments, we can export a
semantic meaningful vectorized curve that connects all involved dashes. Even the raster input contains multiple intersecting dashed curves,
we are able to identify them correctly and individually.
  

<br>

<b>
[<a href="./papers/dashline/dashline.html"-->more</a>]
[<a href="./papers/dashline/dashline.pdf">paper</a>]
[<a href="./papers/dashline/dashline.mp4">presentation video</a>]
[<a href="https://github.com/hyliu/DashedCurveRecognition">code (in preparation)</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/linevector/linevector.html">
  <img src="./papers/linevector/images/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  End-to-End Line Drawing Vectorization</b>
  <i><b> (AAAI 2022)</b></i></font><br> 
  <p>

Line vectorization has long been an open problem. Many approaches have been
proposed, and they usually involve multiple steps that may be proned to
input violating the assumptions. In this project, we propose a neural model
that can take arbitrary line drawing as input and directly export vectorized
output, without the hassles of unstable intermediate steps.

<br>

<b>
[<a
href="./papers/linevector/linevector.html"-->more</a>]
[<a href="./papers/linevector/linevector.pdf">paper</a>]
[<a href="https://github.com/hyliu/LineVec">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/invhalftone/invhalftone.html">
  <img src="./papers/invhalftone/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Invertible Halftoning  </b>
  <i><b> (ICCV 2021)</b></i></font><br> 
  <p>

Once we can store color information in grayscale, the next step is to store
it in halftone. This problem is much harder because of the drastically reduced
solution space of bitonal image (1 bit per pixel). Besides storing the color
information, the binary pattern also serves to preserve the orginal
structural content while avoid to introduce extra visual pattern (blue-noise
property). Also, convolutional neural networks is not good in generating
binary pattern. See how we solve it.


<br>

<b>
[<a href="./papers/invhalftone/invhalftone.html">more</a>]
[<a href="./papers/invhalftone/invhalftone.pdf">paper</a>]
[<a href="./papers/invhalftone/invhalftone_supp.pdf">supplement</a>]
[<a href="./papers/invhalftone/invhalftone_video.mp4">video</a>]
[<a href="https://github.com/MenghanXia/ReversibleHalftoning">code</a>]
[<a href="https://replicate.ai/menghanxia/reversiblehalftoning">demo</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/mangainpaint/mangainpaint.html">
  <img src="./papers/mangainpaint/img/icon-mangainpaint.gif" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Manga Inpainting
  </b> <i><b> (SIGGRAPH 2021)</b></i></font><br>
  <p>
  Manga inpainting has long been needed by the industry during the localization
  and the conversion to electronic manga. But it is mostly done by hand, due to
  the semantically poor  and  seam-obvious inpainting results of existing
  methods. We proposed the first effective deep learning based method for manga
  inpainting that produces semantically meaningful and screentone seamless
  results. Thanks to our previous screenVAE feature, we can avoid the confusion caused by
  the screentone. <br>

<b>
[<a href="./papers/mangainpaint/mangainpaint.html">more</a>]
[<a
href="./papers/mangainpaint/mangainpaint.pdf">paper</a>]
[<a href="./papers/mangainpaint/mangainpaint_supp.pdf">supplement</a>]
[<a href="https://github.com/msxie92/MangaInpainting">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/mangarestore/mangarestore.html">
  <img src="./papers/mangarestore/img/icon-mangarestore.gif" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Exploiting Alaising for Manga Restoration
  </b> <i><b> (CVPR 2021)</b></i></font><br>
  <p>
  Manga images available on the net are usually contaminated with aliasing artifact due
  to the undersampling during the scanning. This is especially true for
  manga, as regular patterns (screentones) are oftenly presented in manga. 
  In fact, such undesired artifact can be exploited as
  a clue to determine the optimal resolution for restoring (superresolving) the
  manga. 
  <br>

<b>
[<a href="./papers/mangarestore/mangarestore.html">more</a>]
[<a href="./papers/mangarestore/mangarestore.pdf">paper</a>]
[<a href="./papers/mangarestore/mangarestore_supp.pdf">supplement</a>]
[<a href="https://github.com/msxie92/MangaRestoration">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/bpnet/bpnet.html">
  <img src="./papers/bpnet/img/icon-bpnet.jpg" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Bidirectional Projection Network
  </b> <i><b> (CVPR 2021)</b></i></font><br>
  <p>
  Existing segmentation methods are mostly unidirectional, i.e. utilizing 3D
  for 2D segmentation or vice versa. Obviously 2D and 3D information can
  nicely complement each other in both directions, during the segmentation. This is the goal of
  bidirectional projection network.
  <br>

<b>
[<a href="./papers/bpnet/bpnet.html">more</a>]
[<a href="./papers/bpnet/bpnet.pdf">paper</a>]
[<a href="https://www.youtube.com/watch?v=Wt9J1l_UBaA">youtube</a>]
[<a href="https://github.com/wbhu/BPNet">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/invbino/invbino.html">
  <img src="papers/invbino/img/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Mononizing Binocular Videos </b>
  <i><b> (SIGGRAPH Asia 2020)</b></i></font><br> 
  <p>

Here we present a fully backward compatible solution to represent a
binocular video as an ordinary monocular video.  So that, it can be played
back, compressed with standard video codec, transmitted, just like any
ordinary monocular video.  The only uniqueness is that it can also be
optionally restored back to its original binocular form, whenever
stereoscopic playback is needed. We achieved this by employing the
InvertibleX Model. In addition, compressing our mononized
video even outperforms the state-of-the-art multiview video encoding.


<br>

<b>
[<a href="./papers/invbino/invbino.html">more</a>]
[<a href="https://youtu.be/rbZR_sF9B5E?list=PL3zJztb9e6XVa266_SeX0Dj66Vjy5f720">youtube</a>]
[<a href="./papers/invbino/invbino.pdf">paper</a>]
[<a href="https://github.com/wbhu/Mono3D">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/screenstyle/screenstyle.html">
  <img src="papers/screenstyle/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Manga Filling with ScreenVAE </b>
  <i><b> (SIGGRAPH Asia 2020)</b></i></font><br> 
  <p>

While automatic converting color comic to screened manga is doable, translating  a bitonal
screened manga to color comic is never done automatically before. The major 
bottleneck lies on the fundamental difference in characterizing how a region
is filled. While a color can be characterized at a single point, a screentone has
to be characterized by a region. To enable the effective automatic 
transation between the color comic and the screened manga, we propose to
unify such fundamental difference, by introducing an intermediate
representation, ScreenVAE map, which converts the region-wise screentone to
a point-wise representation. 

<br>

<b>
[<a href="./papers/screenstyle/screenstyle.html">more</a>]
[<a href="./papers/screenstyle/screenstyle.pdf">paper</a>]
[<a href="https://github.com/msxie92/ScreenStyle">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/embedframe/embedframe.html">
  <img src="./papers/embedframe/img/icon.gif"  border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Video Snapshot</b> (a real live photo)
  <i><b> (IEEE TPAMI 2021)</b></i></font><br> 
  <p>

While iPhone keeps a short video for each "live photo", we propose a method to
embed a short video into a single frame, in other words, a real live photo. We employ
the InvertibleX Model to first encode the neighboring frames into a
single visualizable frame. Whenever the video is needed, a decoding subnetwork
can be used to expand (restore) it.


<br>

<b>
[<a href="./papers/embedframe/embedframe.html">more</a>]
[<a href="https://youtu.be/H5UA5jH4BaQ">youtube</a>]
[<a href="./papers/embedframe/embedframe.pdf"-->paper</a>]
[<a href="https://github.com/chuchienshu/Video-Snapshot">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/deepcvdvideo/deepcvdvideo.html">
  <img src="papers/deepcvdvideo/img/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Color-Consistent and Temporal-Coherent Colorblind-Shareable Videos </b>
  <i><b> (SIGGRAPH Asia 2019)</b></i></font><br> 
  <p>

Due to the local nature of CNN models, it is hard to ensure the recoloring is
consistent over the whole image. To solve this problem in our synthesis of
colorblind-shareable videos, we propose to utilize deep learning in
indirectly generating parameters of a polynomial color model. This
guarantees the recoloring is globally applied to the whole image, while
ensuring temporal coherent.

<br>

<b>
[<a href="./papers/deepcvdvideo/deepcvdvideo.html">more</a>]
[<a href="./papers/deepcvdvideo/deepcvdvideo.pdf">paper</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/invertgray/invertgray.html">
  <img src="papers/invertgray/img/icon.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Invertible Grayscale </b>
  <i><b> (SIGGRAPH Asia 2018)</b></i></font><br> 
  <p>


Here we present a method to convert arbitrary color images to grayscale.
That is not interesting, of course.  The interesting part is that such
grayscale images can be inverted back to color images accurately, without
any guessing.  We present a
learning-based model that offers such color-to-gray conversion
and grayscale-to-color restoration abilities. This is the first realization
of the Invertible Generative Model.

<br>

<b>
[<a href="./papers/invertgray/invertgray.html">more</a>]
[<a href="./papers/invertgray/invertgray.pdf">paper</a>]
[<a href="./papers/invertgray/invertgray.mp4">video</a>]
[<a href="https://github.com/MenghanXia/InvertibelGrayscale">code</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/colorize/colorize.html">
  <img src="papers/colorize/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Two-Stage Sketch Colorization </b>
  <i><b> (SIGGRAPH Asia 2018)</b></i></font><br> 
  <p>

With the advances of neural networks, automatic or semi-automatic
colorization of sketch become feasible and practical. We present a
state-of-the-art semi-automatic (as well as automatic) colorization from
line art. Our improvement is accounted by a divide-and-conquer scheme. We
divide this complex colorization task into two simplier and goal-clearer
subtasks, <i>drafting</i> and <i>refinement</i>.
<br>


<b>
[<a href="./papers/colorize/colorize.html">more</a>]
[<a href="./papers/colorize/colorize.pdf">paper</a>]
[<a href="./papers/colorize/colorize.mp4">video</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/pixel/pixel.html">
  <img src="papers/pixel/img/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Deep Unsupervised Pixelization </b>
  <i><b> (SIGGRAPH Asia 2018)</b></i></font><br> 
  <p>

Generating pixel art from a given input can be regarded as a kind of  style
transfer, and seems to be solvable with existing deep learning.  But the 
real difficulty is the lack of supervised data (thousands of image
pairs of high-resolution input and low-resolution pixel art). This is why we
need an unsupervised learning framework for this pixelization goal.
<br>


<b>
[<a href="./papers/pixel/pixel.html">more</a>]
[<a href="./papers/pixel/pixel.pdf">paper</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/linelearn/linelearn.html">
  <img src="papers/linelearn/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Deep Extraction of Manga Structural Lines </b>
  <i><b> (SIGGRAPH 2017)</b></i></font><br> 
  <p>


Removal of problematic screentone from manga has long been an open problem but
strongly needed, as the digitization process can be significantly simpilified. It is
until the mature of deep learning, we finally be able to remove the irregular,
regular, arbitrarily scaled, or even pictorial screentones with a single unified solution. 

<br>


<b>
[<a href="./papers/linelearn/linelearn.html">more</a>]
[<a href="./papers/linelearn/linelearn.pdf">paper</a>]
[<a href="https://github.com/ljsabc/MangaLineExtraction_PyTorch">software</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/pad/pad.html">
  <img src="./papers/pad/thumbnail/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Pyramid of Arclength Descriptor (PAD)</b>
  <i><b> (SIGGRAPH Asia 2016)</b></i></font><br> 
  <p>

We started with a simple goal, filling space with shapes to exhibit strong
intercoupling appearance, like what M.C. Escher did. But it turns out to be so hard to be practical (tiling
arbitrary shapes within a tractable time). After a decade of research, our
solution is a brand new shape descriptor. It is <i>locally supported</i>,
<i>scale invaraint</i>, suits for <i>partial-shape matching</i>, and more
importantly, <i>efficient to be practical</i>. It will be very useful for many shape recognition problems.
<br>


<b>
[<a href="./papers/pad/pad.html">more</a>]
[<a href="./papers/pad/pad_lowres.pdf">paper</a>]
[<a href="./papers/pad/pad_video.mp4">video</a>]
[<a href="./papers/pad/pad_seq.mp4">video 2</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/colorblind/colorblind.html">
  <img src="papers/colorblind/thumbnail/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Visual Sharing with Colorblinds </b>
  <i><b> (SIGGRAPH 2016)</b></i></font><br> 
  <p>


Modern TV are not designed for colorblinds, leading them hard to share the
same TV with families with normal vision. We propose the first method to
allow colorblinds and normal vision audiences to <i>seamlessly</i> share the same
display, thanks to the wide availability of binocular TV.
<br>


<b>
[<a href="./papers/colorblind/colorblind.html">more</a>]
[<a href="./papers/colorblind/colorblind.pdf">paper</a>]
[<a href="./papers/colorblind/colorblind.wmv">video</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/toontrack/toontrack.html">
  <img src="papers/toontrack/train_icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Globally Optimal Toon Tracking </b>
  <i><b> (SIGGRAPH 2016)</b></i></font><br> 
  <p>



Tracking the corresponding regions throughout a cartoon sequence is
necessary in postprocessing, such as colorization and stereoscopization. 
But it is not available from animators and vision technqiues do not work
well for cartoons.  By formulating the region tracking as a global
optimatization problem and model the region motion trajectory, we can
significantly raise the accuracy to a usable level.  <br>


<b>
[<a href="./papers/toontrack/toontrack.html">more</a>]
[<a href="./papers/toontrack/toontrack.pdf">paper</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/sketch/sketch.html">
  <img src="papers/sketch/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Closure-aware Sketch Simplification </b>
  <i><b> (SIGGRAPH Asia 2015)</b></i></font><br> 
  <p>


Existing methods in simplifying a sketch mainly consider the distance and
orientation similarities of strokes. Thresholding on them usually results in 
unsatisfactory simplifcation. In fact, humans also rely on
the depicting regions in understanding whether individual strokes are
semantically refering the same stroke. But regions are formed by strokes,
and strokes are interpreted by regions. See how we solve the
chicken-or-the-egg problem, by considering the closure gestalts.

<br>

<b>
[<a href="./papers/sketch/sketch.html">more</a>]
[<a href="./papers/sketch/sketch.pdf">paper</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/3dcel/3dcel.html">
  <img src="papers/3dcel/thumbnail/thumbnail3dcel.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Stereoscopizing Cel Animations </b>
  <i><b> (SIGGRAPH Asia 2013)</b></i></font><br> 
  <p>
  While 3D movies are popular nowadays, it is impractical for cel
  animators to hand-draw stereo frames. Geometry modeling cartoon characters
  not only costly, but also requires highly trained skill for lively and
  "organic" presentation of cartoon characters. It would be the best if 
  cel animators remains hand-draw their monocular cels, while leaves our 
  system to automatically turn the cel animations into stereo. 
  
<br>

<b>
[<a href="./papers/3dcel/3dcel.html">more</a>]
[<a href="./papers/3dcel/3dcel.pdf">paper</a>]
[<a href="./papers/3dcel/3dcel_framework.avi">video</a>]
[<a href="./papers/3dcel/3dcel_results.avi">3D video</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/binovision/binovision.html">
  <img src="papers/binovision/thumbnail/thumbbinovision.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Binocular Tone Mapping </b>
  <i><b> (SIGGRAPH 2012)</b></i></font><br> 
  <p>

Tone mapping usually faces a dilemma of looking flat or loosing details. But
what happen if we are given two display domains as in binocular display? We
can present both detail and high-contrast content. The question is whether
these two views can be stably fused into a single percept. This project
proposes a metric to predict such fusability. 

<br>

<b>
[<a href="./papers/binovision/binovision.html">more</a>]
[<a href="./papers/binovision/binovision.pdf">paper</a>]
[<a href="./papers/binovision/binovision.mov">video</a>]
[<a href="./papers/binovision/binovision.zip">program</a>]
[<a href="/press.html#press-bv">press</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/gestalt/gestalt.html">
  <img src="papers/gestalt/thumbnail/thumbgestalt.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Conjoining Gestalt Rules </b>
  <i><b> (SIGGRAPH Asia 2011)</b></i></font><br> 
  <p>

Gestalt laws describe the phenomena of how we humans recognize forms 
(objects), instead of a set of unrelated fragments. By explicitly modeling 
the Gestalt phenomena and their complicated interaction, we are able 
to mimic how humans recognize forms/gestalts. This is a step 
forward to high-level semantic understanding. Interestingly, such 
interpretation is highly related to how artists abstract/simplify the 
image content. 

<br>

<b>
[<a href="./papers/gestalt/gestalt.html">more</a>]
[<a href="./papers/gestalt/gestalt.pdf">paper</a>]
[<a href="./papers/gestalt/gestalt.mov">video</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/puzzle/puzzle.html">
  <img src="./papers/puzzle/thumb/ipuzzle.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
Making Burr Puzzles from 3D Models</b>
  <i><b> (SIGGRAPH 2011)</b></i></font><br> 
  <p>

Burr puzzles have a long history and highly related to anicent Asian 
architecture. Without any screw, burr puzzle pieces can interlock 
each other firmly. Existing burr puzzles usually have more or less 
similar and boring appearance due to the complexity in designing 
puzzle pieces. In this project, we propose a method to break a given 
3D model into burr puzzle pieces. User can control the complexity of 
resultant puzzle by controlling the number of "knots" introduced 
into the model volume. We also show the rapid prototype pieces. 
<br>


<b>

[<a href="./papers/puzzle/puzzle.html">more</a>]
[<a href="./papers/puzzle/puzzle.pdf">paper</a>]
[<a href="./papers/puzzle/puzzle.avi">video</a>] 


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/sumresize/sumresize.html">
  <img src="./papers/sumresize/thumb/isumresize.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
Resizing by Symmetry-Summarization</b>
  <i><b> (SIGGRAPH Asia 2010)</b></i></font><br> 
  <p>

  By understanding the high-level semantics in the image, resizing can be
  done in a semantically meaningful manner. So we propose to analyze the
  symmetry in the image and then <i>summarize</i> the symmetric patterns in 
  order to achieve retargeting. This open a new space for resizing as we can 
  now  reduce/insert image content in a cellwise fashion.

<br>

<b>
[<a href="./papers/sumresize/sumresize.html">more</a>]
[<a href="./papers/sumresize/sumresize.pdf">paper</a>]
[<a href="./papers/sumresize/sumresize.avi">video</a>] 


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/colormood/colormood.html">
  <img src="./papers/colormood/thumb/icolormood.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
Data-Driven Image Color Theme Enhancement</b>
  <i><b> (SIGGRAPH Asia 2010)</b></i></font><br> 
  <p>
Modifying the color theme of a given image is desirable in many
applications. However, it is not trivial as images may not be cartoon-like
(regions of same and clear colors). Here we suggest a way to recolor the
images based on a database and user scribbles. It tries to optimize a result
that confirms to the desired color theme (specified by a
mood/emotion tag), texture, and user constraint.

<br>

<b>
[<a href="./papers/colormood/colormood.html">more</a>]
[<a href="./papers/colormood/colormood.pdf">paper</a>]
[<a href="./papers/colormood/colormood.avi">video</a>] 
[<a href="http://www.youtube.com/watch?v=h0qk7-4Lrd4">youtube</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/asciiart/asciiart.html">
  <img src="papers/asciiart/thumbnails/thumbascii.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Structure-based ASCII Art </b>
  <i><b> (SIGGRAPH 2010)</b></i></font><br> 
  <p>

  It seems that ASCII art has no 
  market value with the wide availability of digital images. 
  Instead, ASCII art becomes even more popular and trendy with 
  the growth of the text-based communication channels, 
  such as SMS and instant messenger. But current ASCII art generation 
  is not satisfactory due to its dithering nature. We propose a method
  to generate structure-based ASCII art that mimics the 
  <i>structure of image content with the structure of 
  characters</i>.

<br>

<b>
[<a href="./papers/asciiart/asciiart.html">more</a>]
[<a href="./papers/asciiart/asciiart.pdf">paper</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/camouflage/camouflage.html">
  <img src="papers/camouflage/thumbnails/thumbcamouflage.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Camouflage Images </b>
  <i><b> (SIGGRAPH 2010)</b></i></font><br> 
  <p>

  Camouflage images are manually created by artists. They intelligently fool
  our human visual perception in acquiring the embedded figure by busy
  texture details. We present a technique that mimics such process to
  generate camouflage images. The importance of this work is not only the 
  generated images, but more importanly, we explore one dimension of our human
  vision.
   

<br>

<b>
[<a href="./papers/camouflage/camouflage.html">more</a>]
[<a href="./papers/camouflage/camouflage.pdf">paper</a>]
[<a href="./papers/camouflage/camouflage.mov">video</a>]
[<a href="http://www.youtube.com/v/s_jIR7s6dpk&hl=zh_TW&fs=1&">youtube</a>]
[<a href="/press.html#press-ci">press</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/maze/maze.html">
  <img src="./papers/maze/images/thumb/thumbmaze.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Evolving Mazes from Images </b>
  <i><b> (IEEE TVCG 2010)</b></i></font><br> 
  <p>

  In daily newspapers and magazines, it is not difficult to find designed
  mazes for readers amusement. Many of these mazes mimic photos, e.g. scenic
  pyramid, animals, plants, etc. Obviously, they are manually designed. In
  this fun project, we turn a given image into a solveable maze
  <i>automatically</i>, by <b>evolving</b> (not designing) the images into
  mazes. It preserves the salient main structure in the original photos, so 
  we can still recognize the underlying photos of the mazes.
  <br>

<b>
[<a href="./papers/maze/maze.html">more</a>]
[<a href="./papers/maze/maze.pdf">paper</a>]
[<a href="./papers/maze/maze.avi">video</a>]
[software]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/flock/flock.html">
  <img src="papers/flock/images/thumbnail/thumbflock.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Animating Animal Motion from Still </b>
  <i><b> (SIGGRAPH Asia 2008)</b></i></font><br> 
  <p>
  Given a single image of animal group, can we animate them without knowing
  how these animals move in prior? In fact, the snapshots of individuals
  embed the "keyframes" of their motion cycle. We developed a way to
  determine the ordering of motion snapshots, so that we can animate the
  motion by morphing.

<br>

<b>
[<a href="./papers/flock/flock.html">more</a>]
[<a href="./papers/flock/flock.pdf">paper</a>]
[<a href="./papers/flock/flock.avi">video</a>]
[<a href="http://www.youtube.com/watch?v=oxJmpbo_jIY">youtube</a>]
[<a href="/press.html#press-flock">press</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/incolor/incolor.html">
  <img src="papers/incolor/images/thumb/thumb-stbasil.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Intrinsic Colorization
  </b> <i><b> (SIGGRAPH Asia 2008)</b></i></font><br>
  <p>

  Traditional colorization directly colorizes on the image without 
  considering the effects of illumination (shadowing and highlight). 
  We demonstrate that such treatment may significantly harm the 
  result. By taking away the illumination and performing the 
  colorization in an intrinsic domain, the interference of 
  illumination can be correctly accounted. To do so, we take advantage 
  of the vast amount of images available on the internet.


<br>

<b>
[<a href="./papers/incolor/incolor.html">more</a>]
[<a href="./papers/incolor/incolor.pdf">paper</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/solid/solid.html">
  <img src="papers/solid/solid_files/thumbsolid.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Solid Texture Synthesis from 2D Exemplars </b>
  <i><b> (SIGGRAPH 2007)</b></i></font><br> 
  <p>

  Solid texturing has been referred as a solution to texturing 
  without distortion and discontinuity for many years. But the 
  difficulties of obtaining 3D textures has not be eased for many 
  years too. It would be perfect if we can just take 2D photo and 
  create the similar 3D solid from it. This is exactly what we do.


<br>

<b>
[<a href="./papers/solid/solid.html">more</a>]
[<a href="./papers/solid/solid.pdf">paper</a>]
[<a href="./papers/solid/solid.mp4">video</a>]
[<a href="http://www.youtube.com/watch?v=KG8kykTNBVk">youtube</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/refilm/refilm.html">
  <img src="./papers/refilm/images/thumb/thumbrefilm.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Refilming with Depth-inferred Videos </b>
  <i><b> (IEEE TVCG 2009)</b></i></font><br> 
  <p>

  Movie special effects (e.g. bullet time) normally cost 
  multi-million dollars to achieve. Can homeusers do that with a single 
  low-cost digital video recorder? We developed an approach to do that, 
  by first estimating the <i>high-quality <b>per-frame</b> depth maps</i> 
  from an ordinary video captured with a handheld camera. Then we can 
  manipulate the depth-inferred video to achieve several special effects 
  on personal computers, like bullet-time, fogging, predator effect, and 
  depth-of-view.  <br>

<b>
[<a href="./papers/refilm/refilm.html">more</a>]
[<a href="./papers/refilm/refilm.pdf">paper</a>]
[<a href="./papers/refilm/refilm.avi">video</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->





<!------------------------------- Computational Manga   ---------------------------------->
<hr>
<a name="manga">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Computational Manga</b></font>
</a>
<hr>

<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/screen/screen.html">
  <img src="papers/screen/images/thumbscreen.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Richness-Preserving Manga Screening </b>
  <i><b> (SIGGRAPH Asia 2008)</b></i></font><br> 
  <p>

  Manga artists draw manga based on real photographs, but unfortunately by
  hand. We present a method to generate mangas from color photographs by
  maintaining the <b>color-distinguishability</b>, texture and tone. As
  screen patterns offer much variety than bitone, why don't we make better
  use of patterns during color-to-bitone conversion?  We try to screen red
  and orange regions with similar patterns, while red and blue regions with
  less similar patterns.  This is what we mean color-distinguisability.

<br>

<b>
[<a href="./papers/screen/screen.html">more</a>]
[<a href="./papers/screen/screen.pdf">paper</a>]
[<a href="./papers/screen/screen.avi">video</a>]
[<a href="http://www.youtube.com/watch?v=MeUS94QATvQ">youtube</a>]
[<a href="/press.html#press-manga">press</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/structurehalftone/structurehalftone.html">
  <img src="papers/structurehalftone/listructurehalftone.png" border=0>
  </a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Structure-Aware Halftoning
  </b> <i><b> (SIGGRAPH 2008)</b></i></font><br>
  <p>
  
   Existing halftoning techniques mainly focus on tone reproduction and
   annoying pattern avoidance. Visually sensitive fine details are usually
   lost during the halftoning. We developed an optimization-based halftoning
   method that can preserve the fine texture as well as the tone similarity.
    <br>


<b>
[<a href="./papers/structurehalftone/structurehalftone.html">more</a>]
[<a href="./papers/structurehalftone/texhalftone.pdf">paper</a>]
[<a href="./papers/structurehalftone/texhalftone.avi">video</a>]</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/illusion/illusion.html">
  <img src="papers/illusion/liillusion.png" border=0>
  </a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Self-Animating Images
  </b> <i><b> (SIGGRAPH 2008)</b></i></font><br>
  <p>

  When certain combination of colors and patterns are used in an 
  image, our human vision system perceives illusion of motion even the 
  image is static. Based on the limited knowledge of this phenomenon,
  we design algorithms to strengthen the illusory effect of motion given
  an input image. Such illusion can be utilized in artwork and design.
  <br>


<b>
[<a href="./papers/illusion/illusion.html">more</a>]
[<a href="./papers/illusion/illusion.pdf">paper</a>]
[<a href="/press.html#press-sai">press</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/manga/manga.html">
  <img src="papers/manga/limanga.jpg" border=0></a><br>
  <font size=1 face="Arial,Helvetica">
      &#169 <a href="http://www.yukito.com">Yukito Kishiro</a>
          / <a href="http://www.shueisha.co.jp">Shueisha</a></font>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Manga Colorization
  </b> <i><b> (SIGGRAPH 2006)</b></i></font><br>
  <p>
  
  The widely popular Japanese manga arts is usually presented in B/W. Its
  intensive use of hatching and screening is a distinctive characteristic,
  but also imposes many difficulties in its digital colorization. Unlike the
  previous colorization methods that rely on intensity-continuity. This
  paper presents a colorization technique that relies on the
  pattern-continuity as well as the intensity-continuity. Hence it can
  naturally colorize the pattern-intensive manga.  <br>

<b>
[<a href="./papers/manga/manga.html">more</a>]
[<a href="./papers/manga/manga.pdf">paper</a>]
[<a href="./papers/manga/manga.avi">video</a>]
[<a href="http://www.youtube.com/watch?v=HpZUOq3O64s">youtube</a>]
[<a href="/press.html#press-manga">press</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/artifact/artifact.html">
  <img src="papers/artifact/liartifact.jpg" border=0></a>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Cartoon Deringing
  </b> <i><b> (ACM TOG 2006)</b></i></font><br>
  <p>
  
  Existing manga (or anime) are normally stored as JPEG (or MPEG). However,
  such BDCT encoding is not suitable for images with sharp edges, like
  manga/anime. Images are usually contaminated with annoying visual
  artifact. This paper presents an analogy-based deringing method to
  <b>synthesize</b> artifact-reduced images instead of traditional
  postfiltering approaches. Substantially visual and statistical
  improvements over existing methods are obtained.
  
 <br>

<b>
[<a href="./papers/artifact/artifact.html">more</a>]
[<a href="./papers/artifact/dering.pdf">paper</a>]
[<a href="./demo/arti/arti.html">software</a>]
[<a href="./papers/artifact/demo.avi">video</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<a name="halftoning">
<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/halftone/sfc.html">
   <img src="papers/halftone/lihalftone.gif" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Details-Preserving Halftoning 
  </b> <i><b> (Graphics Gems V, 1995)</b></i></font><br>
  <p>

<font face="Verdana" size="2"> Halftoning is heavily used in printing
industry.  Classical methods usually exhibit observable patterns. Here, we
make use of the space-filling curves to avoid such pattern. The
space-filling curve turns a 2D problem into 1D. Hence 1D signal processing
techniques can be easily and efficiently applied to retain the details in
the original grayscale input. <br>


<b>
[<a href="./papers/halftone/sfc.html">more</a>]
[<a href="./papers/halftone/sfc.pdf">paper</a>]
[<a href="./software/sfc/sfc.tar.gz">software</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->

</a>





<!----------------------- Weathering  ------------------------------>
<hr>
<a name="weathering">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Weathering</b></font>
</a>
<hr>


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>


   <img src="papers/gammaton/ligammaton.jpg" border=0>


</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  &#611;-ton Tracing
  </b> <i><b> (SIGGRAPH 2005)</b></i></font><br>
  <p>

  &#611;-ton tracing is a visual simulation framework for a wide variety of
  weathering effects. In this framework, blemishes (e.g. rust, stain,
  patina, etc) are formed by the cumulative propagation of an abstract
  weathering-catalysing agent called &#611;-ton. The propagation of
  &#611;-ton resembles the photon mapping. Therefore, once you know how to
  do photon mapping, you can simulate weathering. 
  <br>

<b>
[<a href="./papers/gammaton/gammaton.pdf">paper</a>]
[<a href="/press.html#press-weathering">press</a>]
</b>
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/imperfct/imperfct.html">
   <img src="papers/imperfct/liimperfct.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Geometry-Dependent Surface Imperfections
  </b> <i><b> (EGWR 1997)</b></i></font><br>
  <p>

  The stochastic patterns of real-world weathering phenomena are usually
  geometry-dependent. Example geometric factors include surface exposure, 
  curvature, and external weathering sources (just like the light sources).
  By first determining the underlying tendency from
  scene geometry, we can simulate a wide variety of weathering phenomena.
  <br>

<b>
[<a href="./papers/imperfct/imperfct.html">more</a>]
[<a href="./papers/imperfct/perfect.pdf">paper</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/dust/dust.html">
   <img src="papers/dust/lidust.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Dust Accumulation
  </b> <i><b> (IEEE CGA 1995)</b></i></font><br>
  <p>

  Although weathering phenomena seems to be stochastic, its underlying
  pattern is systematic. Through this specific simulation of dust
  accumulation, we demonstrate the weathering pattern is highly related
  to geometric factor like surface exposure. Here you
  find the <b><i>first</i> ambient occlusion</b> (at that time, I called it 
  <b>surface exposure</b>).
  <br>

<b>
[<a href="./papers/dust/dust.html">more</a>]
[<a href="./papers/dust/dust.pdf">paper</a>]
[<a href="/press.html#press-cgw">press</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->







<!----------------- Sphere Map ---------------------------->
<hr>
<a name="sphere">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Sphere Map</b></font>
</a>
<hr>


<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/isocube/isocube.html">
   <img src="papers/isocube/liisocube.jpg" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Isocube
  </b> <i><b>(IEEE TVCG 2007)</b></i></font><br>
  <p>

  The six-face cubemap is widely equipped on GPU. But the cubemap does not
  sample the sphere uniformly and each texel does not bear the same weight
  (solid angle). Here we propose a new sphere partitioning and sampling
  scheme that  gives uniform sampling (low discrepancy) and equal
  solid-angle properties. Moreover, it fits nicely in the six-face cubemap
  hardware and hence it can steal the hardware antialiasing features of
  cubemap. 
 <br>

<b>
[<a href="./papers/isocube/isocube.html">more</a>]
[<a href="./papers/isocube/isocube.pdf">paper</a>]
[<a href="./papers/isocube/isocube.avi">video</a>]
[<a href="./software/panotransform/panotransform.html">software1</a>]
[<a href="./software/spheremap/spheremap.html">software2</a>]
</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->



<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/rhombic/rhombic.html">
   <img src="papers/rhombic/lirhombic2.png" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Rhombic Dodecahedron Map
  </b> <i><b>(IEEE TMM 2009)</b></i></font><br>
  <p>

  This proposed rhombic dodecahedron map partitions the sphere into 
  12 <b>equal</b> base faces (rhombi, equal in shape and solid angle). 
  Although the partitioned texels on each base face may not have the 
  same solid angle (but very close), this allows us to trade for 
  uniformity in shape distortion, that cannot be maintained in 
  isocube and HEALPix. 

 <br>

<b>
[<a href="./papers/rhombic/rhombic.html">more</a>]
[<a href="./papers/rhombic/rhombic.pdf">paper</a>]
[<a href="./software/panotransform/panotransform.html">software</a>]

</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->


<a name="sampling">
<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/q2tree/q2tree.html">
   <img src="papers/hpsamp2/lihpsamp2.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Spherical Q2-tree</b> <i><b>(EGSR 2005)</b></i></font><br>
  <p>

  To speed up the rendering, we can approximate the distant envrionment
  lighting by a finite number of directional lights. The question is how to
  sample the spherical environment map. By developing the 360 degree
  environment onto a plane with solid angle equality, we then turn the
  importance sampling problem into a simple quad-tree partitioning problem
  on sphere.<br>

<b>
[<a href="./papers/q2tree/q2tree.html">more</a>]
[<a href="./papers/q2tree/q2tree.pdf">paper</a>]
[<a href="./papers/q2tree/image/q2tree-video.avi">video</a>]
[<a href="./software/panotransform/panotransform.html">software1</a>]
[<a href="./software/spheremap/spheremap.html">software2</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->

</a>


<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/udpoint/udpoints.html">
   <img src="papers/udpoint/icon/lihamm.png" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Hammersley and Halton Points
  </b> <i><b>(JGT 1997)</b></i></font><br>
  <p>

  With a set of simple equations, we can generate uniformly distributed
  (low-discrepancy) but deterministic sample points over the spherical
  surface. The total number of sample points can be arbitrarily specified.
  All these features make it a neat and handy tool for sphere sampling.

 <br>

<b>
[<a href="./papers/udpoint/udpoints.html">more</a>]
[<a href="./papers/udpoint/udpoint.pdf">paper</a>]
[<a href="./papers/udpoint/udpoint.zip">software</a>]

</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->









<!--------------------- Relighting --------------------------------->
<hr>
<a name="relight">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Relighting and Precomputed Lighting</b></font>
</a>
<hr>


<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/btftile/btftile.html">
   <img src="papers/btftile/libtftile.jpg" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Tileable BTF
  </b> <i><b>(IEEE TVCG 2007)</b></i></font><br>
  <p>

  Dressing an object with BTF normally involves a geometry-dependent
  synthesis. The synthesized BTF cannot be reused on another object. Since
  the synthesis of high-dimensional BTF is very time-consuming, this hinders
  the application of BTF. We proposed the tileable BTF to decouple the BTF
  synthesis and the geometry. Hence, once a set of BTF tiles is synthesized, 
  it can be repeatedly applied on various objects without the lengthy
  synthesis. The dressing is instantaneous. <br>

<b>
[<a href="./papers/btftile/btftile.html">more</a>]
[<a href="./papers/btftile/btftile.pdf">paper</a>]
[<a href="./papers/btftile/btftile.avi">video</a>]
</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->



<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/shfit/shfit.html">
   <img src="papers/shfit/lishfit.jpg" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Noise-Proofed SH
  </b> <i><b>(IEEE TVCG 2006)</b></i></font><br>
  <p>

  Storing precomputed lighting information (such as relighting, BTF, and
  PRT) with spherical harmonics (SH) can drastically reduce the data size.
  However, that is still far away from practical use as each vertex or pixel
  has its own SH coefficient vector (of, say, 25 x 25 elements). Further
  compression on these SH coefficients may carelessly introduce severe
  visual artifacts.  We digged up the underlying reason and introduced a way
  to <i>noise-proof</i> the SH coefficients, hence practical applications
  become feasible.<br>

<b>
[<a href="./papers/shfit/shfit.html">more</a>]
[<a href="./papers/shfit/shfit.pdf">paper</a>]
</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->



<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <!a href="./papers/stereobp2/stereobp2.html">
   <img src="papers/stereobp2/listereobp2.jpg" border=0>
   <!/a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Dense Photometric Stereo
  </b> <i><b>(IEEE TPAMI)</b></i></font><br>
  <p>

Given the massive number of images captured for relighting, we can
accurately recover the normal and depth maps using <i>dense photometric
stereo</i>. Recovery is accurate even the lighting direction is not
precisely measured and there are substantial amount of shadow and specular
highlight (which causes much troubles to traditional shape recovery
algorithms).  Note that the <i>same</i> set of images is used for depth
recovery and relighting. No range scanner is required. <br>

<b>
[<a href="./papers/stereobp2/stereobp2.pdf">paper</a>]
[<a href="./papers/stereobp2/stereobp2.avi">video</a>]
</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/pca/pca.html">
   <img src="papers/pca/lipca.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  PCA Compression for Relighting
  </b> <i><b> (IEEE CSVT 2005)</b></i></font><br>
  <p>

Previous methods in compression relighting data mostly rely on SH which may
smooth out high-frequency details. Here, we propose a block-based PCA
compression scheme. It essentially separates the data into basis images and
relighting coefficients, which can be further effectively compressed using
transform coding and quantization. The nice feature of PCA is its ability in
preserving high-quality shadow and highlights, while maintaining high
compression ratio and fast rendering.

<br>

<b>
[<a href="./papers/pca/pca.html">more</a>]
[<a href="./papers/pca/pca.pdf">paper</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/plenwave/plenwave.html">
   <img src="papers/plenwave/liplenwave.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Compression for Relighting
  </b> <i><b> (IEEE CSVT 2003)</b></i></font><br>
  <p>

Relighting or precomputed lighting is actually a compression problem of
tons of images. One may straightforwardly compress them with image or
video compression standards such as JPEG, JPEG2000 and MPEG. However, they
do not work well nor fully exploit the data correlation. Unlike the
frame-by-frame playback of video, relighting may <i>randomly access</i> the
pixel values during image synthesis. This paper presents a
divide-and-conquer compression approach which substantially outperforms
existing image and video coding methods. <br>

<b>
[<a href="./papers/plenwave/plenwave.html">more</a>]
[<a href="./papers/plenwave/plenwave.pdf">paper</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/panoshad/panoshad.html">
   <img src="papers/panoshad/lipanoshad.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Real-Time Relighting
  </b> <i><b> (Graphics Programming Method 2003)</b></i></font><br>
  <p>

Relighting in local illumination (directional and point light sources) is
basically a linear combination of basis functions, either image-wise or
pixel-wise. The grid nature of image-based relighting fits nicely into the
current SIMD-based GPUs and facilitates its <i>real-time</i> rendering.
However, SH imposes difficulties due to its recursive definition. To solve
it, we utilize hardware cubemaps.
<br>

<b>
[<a href="./papers/panoshad/panoshad.html">more</a>] 
[<a href="./papers/panoshad/panoshad.pdf">paper</a>]
[<a href="./software/panoshader/panoshader.html">software</a>] 

</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

  <a href="./papers/illusam2/illusam2.html">
  <img src="papers/illusam2/liillusam2.jpg" border=0>
  </a>

</td><td width=20>
</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Sampling Bound of Relighting
  </b> <i><b> (CVPR 2001) (IJCV 2002)</b></i></font><br>
  <p>

  How many samples we should take along the lighting dimension in order to
  synthesize realistic images? If we know the answer, we can avoid wasteful
  oversampling and undesirable undersampling, during the relighting and
  precomputed lighting. This is exactly what this paper trying to answer. We
  went through an indepth analysis and obtained the theoretical sampling
  bound for relighting and precomputed lighting. To verify our bound, we
  even build a machine to precisely position the point light source.<br>
  
<b>
[<a href="./papers/illusam2/illusam2.html">more</a>]
[<a href="./papers/illusam2/illusam2.pdf">paper</a>]
[<a href="./software/irrad/irrad.html">software</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/plenill2/plenill2.html">
   <img src="papers/plenill2/liplenill2.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  The Plenoptic Illumination Function
  </b> <i><b> (IEEE TMM 2002)</b></i></font><br>
  <p>

The original formulation of the plenoptic function does not include
illumination. Previous image-based virtual reality applications simply
assume that the illumination is fixed. In this paper, we proposed a new
formulation of the plenoptic function, called the <i>plenoptic illumination
function</i>, which explicitly specifies the illumination component.
Techniques based on this new formulation can be extended to support
relighting as well as view interpolation. This paper received the <i> <a
href="http://www.ieee.org/organizations/society/sp/tmmaward.html">IEEE
Transactions on Multimedia Prize Paper Award 2005</a></i>.<br>

<b>
[<a href="./papers/plenill2/plenill2.html">more</a>] 
[<a href="./papers/plenill2/plenill2.pdf">paper</a>]
[<a href="./software/panoview/panoview.html">software</a>]
[<a href="/press.html#press-ibr">press</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/lightfld/lightfld.html">
   <img src="papers/lightfld/lilightfld.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  ABRDF and Image-based Relighting 
  </b> <i><b> (EGWR 1997)</b></i></font><br>
  <p>
 
  The earliest paper on image-based relighting from our group in 1997. We
  attempted to include the lighting into the image-based rendering framework. 
  So that we are able to change the viewpoint as well as the lighting.
  Our work is later commonly referred as <i>image-based
  relighting</i>.  In this paper, we introduced the concept of
  <i>apparent BRDF (ABRDF)</i>  of pixel, which is the aggregate function 
  of surface reflectance, shadowing, and other indirect illumination
  contribution.
  <br>


<b>
[<a href="./papers/lightfld/lightfld.html">more</a>]
[<a href="./papers/lightfld/illumin.pdf">paper</a>]
[<a href="./papers/lightfld/ptsrc.mpg">video</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->





<!----------------- View Interpolation ---------------------------->
<hr>
<a name="view-interpolation">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>View Interpolation</b></font>
</a>
<hr>

<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/bspi/bspi.html">
   <img src="papers/bspi/libspi.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Binary-Space-Partitioned Images</b> <i><b>(IEEE TVCG 2004)</b></i></font><br>
  <p>


<font face="Verdana" size="2"> Can we resolve the visibility without
referring to 3D? This paper tries to answer this question. By binary-space
partitioning the input 2D image and projecting the novel viewpoint onto the
view plane of the input image, novel views can be generated. The 3D
visibility can be correctly and completely resolved in 2D. All we need is a
2D BSP visibility sorting. <br>

<b>
[<a href="./papers/bspi/bspi.html">more</a>]
[<a href="./papers/bspi/bspi.pdf">paper</a>]
[<a href="./papers/bspi/animation/bspi-720-480-mpeg2.mpg">video</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/triorder/triorder.html">
   <img src="papers/triorder/litriorder.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  View Interpolation Without Depth-Buffering
  </b> <i><b> (JGT 1998)</b></i></font><br> 
  <p>


Using epipolar geometry, a visibility-correct drawing order of <i>pixels</i>
can be derived without depth-buffering. The drawback is to do it
pixel-by-pixel. Again, using epipolar geometry, we derive a
visibility-correct drawing order for <i>2D triangles</i>. The order between
each pair of neighboring triangles is first obtained. Then a graph can be
built and topological sorting is applied on the graph to obtain the complete
drawing order of all triangles in linear time.
<br> 

<b>
[<a href="./papers/triorder/triorder.html">more</a>]
[<a href="./papers/triorder/triorder.pdf">paper</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->











<!--------------------------- GPGPU  ----------------------------------->
<hr>
<a name="gpgpu">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>GPGPU</b></font>
</a>
<hr>


<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/dwtgpu/dwtgpu.html">
   <img src="papers/dwtgpu/icon/lidwtgpu.jpg" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  DWT on GPU
  </b> <i><b>(IEEE TMM 2007)</b></i></font><br>
  <p>

  Discrete wavelet transform (DWT) has been widely used in recent years,
  e.g. JPEG2000 image coding standard.  But DWT is rather computational
  intensive when the data size is large.  Since most PCs come with a GPU, why
  don't we turn it into a powerful DWT engine? As there are many DWTs, we
  proposed a method to support various separable DWTs with various boundary
  extension schemes. This project started in 2003 and its source
  code has been released since 2004.  <br>

<b>
[<a href="./papers/dwtgpu/dwtgpu.html">more</a>]
[<a href="./papers/dwtgpu/dwtgpu.pdf">paper</a>]
[<a href="./software/dwtgpu/dwtgpu.html">software</a>]
</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/ecgpu/ecgpu.html">
   <img src="papers/ecgpu/liecgpu.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Evolutionary Computing on GPU
  </b> <i><b> (IEEE IS)</b></i></font><br>
  <p>


Genetic algorithm is a kind of evolutionary algorithms. Evolutionary
algorithms are effective and robust methods for solving many practical
problems such as feature selection, electrical circuits synthesis, and data
mining. However, they may execute for a long time, because enormous fitness
evaluations must be performed. In this paper, we proposed the first
evolutionary algorithm on GPU, affordable by most people.<br>

<b>
[<a href="./papers/ecgpu/ecgpu.html">more</a>]
[<a href="./papers/ecgpu/ecgpu.pdf">paper</a>]
[<a href="./software/ecgpu/ecgpu.html">software</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->














<!--------------------- Volume Rendering -------------------------->
<hr>
<a name="volume-rendering">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Visualization</b></font>
</a>
<hr>

<p>
<table border=0><tr><td width=80 height=80 align=right valign=top>

   <a href="./papers/asc/asc.html">
   <img src="papers/asc/liasc.jpg" border=0>
   </a>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Adaptive Skeleton Climbing
  </b> <i><b> (Eurographics 1998)</b></i></font><br>
  <p>


<font face="Verdana" size="2"> Generating isosurfaces from volume data
normally results in large amount of triangles. One can use post-processing
mesh decimation technique to reduce the triangle count, but with the
trade-off of long processing time. To address this problem, we proposed an
isosurface extraction algorithm that generates decimated mesh
<i>directly</i> from the volume in a <i>short</i> period of running time.
<br>

<b>
[<a href="./papers/asc/asc.html">more</a>]
[<a href="./papers/asc/asc.pdf">paper</a>]
[<a href="./software/asc/asc-intro.html">software</a>]
</b>

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->





















<br>
</font>


</blockquote>
</HTML>
